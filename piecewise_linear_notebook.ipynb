{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Piecewise Linearity in Neural Networks: Interactive Demonstration\n",
    "\n",
    "This notebook demonstrates one of the most fundamental properties of modern deep learning models: **piecewise linearity**.\n",
    "\n",
    "## Key Insight\n",
    "\n",
    "Neural networks with ReLU-like activation functions produce functions that are composed of multiple linear pieces, stitched together at boundaries where neurons switch from active to inactive.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib for better looking plots\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"✓ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Simple 1D Example\n",
    "\n",
    "Let's start with the simplest possible example: a 1D neural network with just 3 hidden neurons.\n",
    "\n",
    "### Network Architecture\n",
    "- **Input**: 1D (a single number)\n",
    "- **Hidden layer**: 3 neurons with ReLU activation\n",
    "- **Output**: 1D (a single number)\n",
    "\n",
    "Each neuron in the hidden layer activates at a different threshold, creating distinct linear regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tiny_network(x):\n",
    "    \"\"\"\n",
    "    A simple 1D network: input -> 3 ReLU neurons -> output\n",
    "    \n",
    "    Hidden layer neurons:\n",
    "      h1 = max(0, x + 1)  # Activates when x > -1\n",
    "      h2 = max(0, x)       # Activates when x > 0\n",
    "      h3 = max(0, x - 1)   # Activates when x > 1\n",
    "    \n",
    "    Output: 0.5*h1 - 1.0*h2 + 0.5*h3\n",
    "    \"\"\"\n",
    "    h1 = np.maximum(0, x + 1)\n",
    "    h2 = np.maximum(0, x)\n",
    "    h3 = np.maximum(0, x - 1)\n",
    "    \n",
    "    output = 0.5 * h1 - 1.0 * h2 + 0.5 * h3\n",
    "    return output\n",
    "\n",
    "# Test the network\n",
    "test_input = 0.5\n",
    "test_output = tiny_network(test_input)\n",
    "print(f\"Network input: {test_input}\")\n",
    "print(f\"Network output: {test_output:.4f}\")\n",
    "print(\"\\n✓ Network defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Piecewise Linear Function\n",
    "\n",
    "Now let's visualize this function and see the piecewise linear structure clearly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data points\n",
    "x = np.linspace(-2, 2, 1000)\n",
    "y = tiny_network(x)\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Plot the function\n",
    "ax.plot(x, y, linewidth=3, color='darkblue', label='Network Output')\n",
    "\n",
    "# Mark the activation boundaries\n",
    "ax.axvline(-1, color='red', linestyle='--', alpha=0.7, linewidth=2, label='Neuron 1 activates (x = -1)')\n",
    "ax.axvline(0, color='green', linestyle='--', alpha=0.7, linewidth=2, label='Neuron 2 activates (x = 0)')\n",
    "ax.axvline(1, color='purple', linestyle='--', alpha=0.7, linewidth=2, label='Neuron 3 activates (x = 1)')\n",
    "\n",
    "# Highlight the \"kinks\" where the function changes slope\n",
    "kink_points = [-1, 0, 1]\n",
    "kink_values = [tiny_network(np.array([k]))[0] for k in kink_points]\n",
    "ax.scatter(kink_points, kink_values, color='red', s=100, zorder=5, label='Kink points')\n",
    "\n",
    "# Labels and styling\n",
    "ax.set_xlabel('Input (x)', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Output', fontsize=14, fontweight='bold')\n",
    "ax.set_title('Piecewise Linear Function from ReLU Network\\n(Notice the kinks at activation boundaries)', \n",
    "             fontsize=16, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(loc='best', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice how the function is perfectly linear between the dashed lines!\")\n",
    "print(\"Each section has a constant slope - that's what 'piecewise linear' means.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Each Linear Region\n",
    "\n",
    "Let's break down what's happening in each region:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"LINEAR REGIONS ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nFor network: output = 0.5*max(0,x+1) - 1.0*max(0,x) + 0.5*max(0,x-1)\\n\")\n",
    "\n",
    "regions = [\n",
    "    (\"Region 1: x < -1\", \"All neurons OFF\", \"output = 0\", 0),\n",
    "    (\"Region 2: -1 ≤ x < 0\", \"Only neuron 1 ON\", \"output = 0.5(x+1) = 0.5x + 0.5\", 0.5),\n",
    "    (\"Region 3: 0 ≤ x < 1\", \"Neurons 1,2 ON\", \"output = 0.5(x+1) - 1.0x = -0.5x + 0.5\", -0.5),\n",
    "    (\"Region 4: x ≥ 1\", \"All neurons ON\", \"output = 0.5(x+1) - 1.0x + 0.5(x-1) = 0\", 0)\n",
    "]\n",
    "\n",
    "for region_name, neurons, formula, slope in regions:\n",
    "    print(f\"{region_name}\")\n",
    "    print(f\"  Activation: {neurons}\")\n",
    "    print(f\"  Formula: {formula}\")\n",
    "    print(f\"  Slope: {slope}\")\n",
    "    print()\n",
    "\n",
    "print(\"Key insight: Within each region, the function is EXACTLY LINEAR.\")\n",
    "print(\"The overall function is made up of these linear pieces!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification: Computing Derivatives\n",
    "\n",
    "If the function is truly piecewise linear, the first derivative (slope) should be constant within each region, and the second derivative should be zero (except at the boundaries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute numerical derivatives\n",
    "dy_dx = np.gradient(y, x)  # First derivative (slope)\n",
    "d2y_dx2 = np.gradient(dy_dx, x)  # Second derivative (curvature)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: First derivative (slope)\n",
    "ax1 = axes[0]\n",
    "ax1.plot(x, dy_dx, linewidth=2.5, color='darkred', label='First Derivative (slope)')\n",
    "ax1.axvline(-1, color='red', linestyle='--', alpha=0.5)\n",
    "ax1.axvline(0, color='green', linestyle='--', alpha=0.5)\n",
    "ax1.axvline(1, color='purple', linestyle='--', alpha=0.5)\n",
    "ax1.axhline(0, color='black', linestyle='-', alpha=0.2)\n",
    "ax1.set_xlabel('Input (x)', fontsize=12)\n",
    "ax1.set_ylabel('dy/dx', fontsize=12)\n",
    "ax1.set_title('First Derivative: Constant Within Each Region\\n(This proves the function is piecewise linear!)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend(fontsize=11)\n",
    "\n",
    "# Plot 2: Second derivative (curvature)\n",
    "ax2 = axes[1]\n",
    "ax2.plot(x, d2y_dx2, linewidth=2.5, color='darkgreen', label='Second Derivative (curvature)')\n",
    "ax2.axvline(-1, color='red', linestyle='--', alpha=0.5, label='Activation boundaries')\n",
    "ax2.axvline(0, color='green', linestyle='--', alpha=0.5)\n",
    "ax2.axvline(1, color='purple', linestyle='--', alpha=0.5)\n",
    "ax2.axhline(0, color='black', linestyle='-', alpha=0.2)\n",
    "ax2.set_xlabel('Input (x)', fontsize=12)\n",
    "ax2.set_ylabel('d²y/dx²', fontsize=12)\n",
    "ax2.set_title('Second Derivative: Zero Except at Boundaries\\n(Zero curvature = linear function)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.set_ylim(-0.5, 0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ The second derivative is essentially zero within each region!\")\n",
    "print(\"✓ This mathematically proves the function is piecewise linear.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: 2D Neural Network Example\n",
    "\n",
    "Now let's look at a more realistic example with 2D inputs. This creates a piecewise linear **surface** instead of a curve.\n",
    "\n",
    "### Network Architecture\n",
    "- **Input**: 2D (two numbers)\n",
    "- **Hidden layer**: 4 neurons with ReLU activation\n",
    "- **Output**: 1D (a single number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network_2d(X, W1, b1, W2, b2):\n",
    "    \"\"\"\n",
    "    2-layer network: input(2D) -> hidden(4 neurons, ReLU) -> output(1D)\n",
    "    \n",
    "    Args:\n",
    "        X: Input array of shape (n_samples, 2)\n",
    "        W1: First layer weights, shape (2, 4)\n",
    "        b1: First layer biases, shape (4,)\n",
    "        W2: Second layer weights, shape (4, 1)\n",
    "        b2: Second layer bias, shape (1,)\n",
    "    \n",
    "    Returns:\n",
    "        Output array of shape (n_samples, 1)\n",
    "    \"\"\"\n",
    "    # First layer with ReLU activation\n",
    "    hidden = np.maximum(0, X @ W1 + b1)\n",
    "    \n",
    "    # Output layer (linear)\n",
    "    output = hidden @ W2 + b2\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Initialize random weights\n",
    "np.random.seed(42)\n",
    "W1 = np.random.randn(2, 4) * 0.5\n",
    "b1 = np.random.randn(4) * 0.5\n",
    "W2 = np.random.randn(4, 1) * 0.5\n",
    "b2 = np.random.randn(1) * 0.5\n",
    "\n",
    "print(\"✓ 2D Neural network initialized!\")\n",
    "print(f\"\\nNetwork architecture:\")\n",
    "print(f\"  Input dimension: 2\")\n",
    "print(f\"  Hidden layer: 4 neurons with ReLU\")\n",
    "print(f\"  Output dimension: 1\")\n",
    "print(f\"\\nMaximum possible linear regions: 2^4 = {2**4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Piecewise Linear Surface\n",
    "\n",
    "Let's generate a grid of input points and compute the network output for each point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid of input points\n",
    "x1_range = np.linspace(-2, 2, 150)\n",
    "x2_range = np.linspace(-2, 2, 150)\n",
    "X1, X2 = np.meshgrid(x1_range, x2_range)\n",
    "\n",
    "# Flatten for computation\n",
    "X_flat = np.column_stack([X1.ravel(), X2.ravel()])\n",
    "\n",
    "# Compute network output for all points\n",
    "Y_flat = neural_network_2d(X_flat, W1, b1, W2, b2)\n",
    "Y = Y_flat.reshape(X1.shape)\n",
    "\n",
    "print(f\"✓ Computed network output for {len(X_flat):,} points\")\n",
    "print(f\"  Grid size: {X1.shape[0]} x {X1.shape[1]}\")\n",
    "print(f\"  Output range: [{Y.min():.3f}, {Y.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D Visualization: The Piecewise Linear Surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3D surface plot\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot the surface\n",
    "surf = ax.plot_surface(X1, X2, Y, cmap=cm.viridis, \n",
    "                       alpha=0.9, linewidth=0, antialiased=True,\n",
    "                       edgecolor='none')\n",
    "\n",
    "# Add contour lines at the base\n",
    "ax.contour(X1, X2, Y, zdir='z', offset=Y.min()-0.5, cmap=cm.viridis, alpha=0.5)\n",
    "\n",
    "# Labels and title\n",
    "ax.set_xlabel('Input x₁', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Input x₂', fontsize=12, fontweight='bold')\n",
    "ax.set_zlabel('Network Output', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Piecewise Linear Surface from 2D Neural Network\\n'\n",
    "             'The surface is made of flat planar regions stitched together', \n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "# Add colorbar\n",
    "fig.colorbar(surf, ax=ax, shrink=0.5, aspect=5)\n",
    "\n",
    "# Set viewing angle\n",
    "ax.view_init(elev=25, azim=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Look closely at the surface - you can see the flat planar regions!\")\n",
    "print(\"Each flat region corresponds to a different activation pattern of the 4 hidden neurons.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contour Plot: Seeing the Linear Regions\n",
    "\n",
    "A contour plot makes the linear regions even more visible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create contour plot\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Filled contours\n",
    "contourf = ax.contourf(X1, X2, Y, levels=30, cmap='viridis', alpha=0.8)\n",
    "\n",
    "# Contour lines\n",
    "contour = ax.contour(X1, X2, Y, levels=15, colors='white', \n",
    "                     linewidths=1, alpha=0.6)\n",
    "ax.clabel(contour, inline=True, fontsize=8, fmt='%.2f')\n",
    "\n",
    "# Labels and title\n",
    "ax.set_xlabel('Input x₁', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Input x₂', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Contour Plot: Linear Regions Are Visible\\n'\n",
    "             'Straight contour lines indicate linear regions', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, linestyle='--')\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "# Colorbar\n",
    "cbar = plt.colorbar(contourf, ax=ax)\n",
    "cbar.set_label('Network Output', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice how the contour lines are STRAIGHT within each region!\")\n",
    "print(\"This is a telltale sign of piecewise linearity.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Section Analysis\n",
    "\n",
    "Let's take a slice through the surface to see the piecewise linearity more clearly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take cross-sections at different x2 values\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "x2_values = [-1.5, -0.5, 0.5, 1.5]\n",
    "\n",
    "for i, x2_fixed in enumerate(x2_values):\n",
    "    # Find the index closest to x2_fixed\n",
    "    idx = np.argmin(np.abs(x2_range - x2_fixed))\n",
    "    cross_section = Y[idx, :]\n",
    "    \n",
    "    # Plot\n",
    "    axes[i].plot(x1_range, cross_section, linewidth=2.5, color='darkblue')\n",
    "    axes[i].set_xlabel('Input x₁', fontsize=11)\n",
    "    axes[i].set_ylabel('Network Output', fontsize=11)\n",
    "    axes[i].set_title(f'Cross-section at x₂ = {x2_fixed:.1f}', fontsize=12, fontweight='bold')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add note\n",
    "    axes[i].text(0.05, 0.95, 'Piecewise linear!', \n",
    "                transform=axes[i].transAxes, \n",
    "                fontsize=10, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.3))\n",
    "\n",
    "plt.suptitle('Cross-Sections Through the 2D Surface\\nEach slice is clearly piecewise linear', \n",
    "             fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Every cross-section through the surface is piecewise linear!\")\n",
    "print(\"This holds true regardless of which direction we slice.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Numerical Verification\n",
    "\n",
    "Let's verify piecewise linearity numerically by checking that the second derivative is zero within regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_linearity_along_line(start, end, W1, b1, W2, b2, n_points=100):\n",
    "    \"\"\"\n",
    "    Check if the network function is linear along a line segment.\n",
    "    Returns the maximum second difference (should be ~0 for linear segments).\n",
    "    \"\"\"\n",
    "    # Generate points along the line\n",
    "    t = np.linspace(0, 1, n_points)[:, np.newaxis]\n",
    "    points = start + t * (end - start)\n",
    "    \n",
    "    # Evaluate network\n",
    "    outputs = neural_network_2d(points, W1, b1, W2, b2).ravel()\n",
    "    \n",
    "    # Compute second differences (discrete approximation of second derivative)\n",
    "    first_diff = np.diff(outputs)\n",
    "    second_diff = np.diff(first_diff)\n",
    "    \n",
    "    max_second_diff = np.max(np.abs(second_diff))\n",
    "    return max_second_diff\n",
    "\n",
    "# Test multiple random line segments\n",
    "print(\"=\"*70)\n",
    "print(\"NUMERICAL VERIFICATION OF PIECEWISE LINEARITY\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nTesting linearity along random line segments:\")\n",
    "print(\"(If max second difference ≈ 0, the function is linear in that region)\\n\")\n",
    "\n",
    "np.random.seed(123)\n",
    "n_tests = 10\n",
    "linear_count = 0\n",
    "\n",
    "for i in range(n_tests):\n",
    "    start = np.random.randn(2) * 2\n",
    "    end = np.random.randn(2) * 2\n",
    "    max_sd = check_linearity_along_line(start, end, W1, b1, W2, b2)\n",
    "    \n",
    "    is_linear = max_sd < 1e-8\n",
    "    if is_linear:\n",
    "        linear_count += 1\n",
    "    \n",
    "    status = \"✓ LINEAR\" if is_linear else \"✗ Contains boundaries\"\n",
    "    print(f\"Test {i+1:2d}: max |d²y| = {max_sd:.2e}  {status}\")\n",
    "\n",
    "print(f\"\\nResults: {linear_count}/{n_tests} line segments were perfectly linear\")\n",
    "print(f\"         {n_tests - linear_count}/{n_tests} segments crossed activation boundaries\")\n",
    "print(\"\\n✓ This confirms the piecewise linear structure!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Activation Pattern Analysis\n",
    "\n",
    "Each linear region corresponds to a unique activation pattern - which neurons are \"on\" (active) vs \"off\" (inactive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ACTIVATION PATTERN ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nNetwork has {W1.shape[1]} hidden neurons\")\n",
    "print(f\"Maximum possible activation patterns: 2^{W1.shape[1]} = {2**W1.shape[1]}\")\n",
    "print(f\"Each pattern corresponds to a different linear region\\n\")\n",
    "\n",
    "# Sample points from different regions\n",
    "sample_points = np.array([\n",
    "    [-2.0, -2.0],\n",
    "    [-1.0, -1.0],\n",
    "    [0.0, 0.0],\n",
    "    [1.0, 1.0],\n",
    "    [2.0, 2.0],\n",
    "    [-1.5, 1.5],\n",
    "    [1.5, -1.5]\n",
    "])\n",
    "\n",
    "print(\"Sample points and their activation patterns:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Point':>15} | {'Pattern':>12} | {'Output':>8}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for point in sample_points:\n",
    "    # Compute hidden layer activations\n",
    "    hidden = np.maximum(0, point @ W1 + b1)\n",
    "    \n",
    "    # Binary activation pattern (1 = active, 0 = inactive)\n",
    "    pattern = (hidden > 1e-10).astype(int)\n",
    "    pattern_str = ''.join(map(str, pattern))\n",
    "    \n",
    "    # Network output\n",
    "    output = neural_network_2d(point.reshape(1, -1), W1, b1, W2, b2)[0, 0]\n",
    "    \n",
    "    print(f\"({point[0]:>5.1f}, {point[1]:>5.1f}) | {pattern_str:>12} | {output:>8.3f}\")\n",
    "\n",
    "print(\"-\" * 70)\n",
    "print(\"\\nKey insight: Points with the same activation pattern are in the same\")\n",
    "print(\"             linear region and follow the same linear equation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Interactive Exploration (Optional)\n",
    "\n",
    "Try modifying the network parameters and see how the piecewise linear structure changes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new network with different random weights\n",
    "np.random.seed(999)  # Try different seeds: 42, 123, 999, etc.\n",
    "\n",
    "W1_new = np.random.randn(2, 6) * 0.5  # Try more neurons!\n",
    "b1_new = np.random.randn(6) * 0.5\n",
    "W2_new = np.random.randn(6, 1) * 0.5\n",
    "b2_new = np.random.randn(1) * 0.5\n",
    "\n",
    "# Compute new surface\n",
    "Y_new_flat = neural_network_2d(X_flat, W1_new, b1_new, W2_new, b2_new)\n",
    "Y_new = Y_new_flat.reshape(X1.shape)\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "surf = ax.plot_surface(X1, X2, Y_new, cmap=cm.plasma, \n",
    "                       alpha=0.9, linewidth=0, antialiased=True)\n",
    "\n",
    "ax.set_xlabel('Input x₁', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Input x₂', fontsize=12, fontweight='bold')\n",
    "ax.set_zlabel('Network Output', fontsize=12, fontweight='bold')\n",
    "ax.set_title(f'Different Network with {W1_new.shape[1]} Hidden Neurons\\n'\n",
    "             f'Still piecewise linear, but with more regions!', \n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "fig.colorbar(surf, ax=ax, shrink=0.5)\n",
    "ax.view_init(elev=25, azim=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ This network has {W1_new.shape[1]} hidden neurons\")\n",
    "print(f\"  Maximum possible regions: 2^{W1_new.shape[1]} = {2**W1_new.shape[1]}\")\n",
    "print(\"  More neurons = more linear regions = more complex function!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Key Takeaways\n",
    "\n",
    "### What We've Learned\n",
    "\n",
    "1. **ReLU networks are piecewise linear**: The entire function is composed of linear pieces stitched together\n",
    "\n",
    "2. **Activation boundaries create regions**: Each neuron's activation threshold creates a boundary between linear regions\n",
    "\n",
    "3. **Activation patterns define regions**: Each unique combination of active/inactive neurons corresponds to one linear region\n",
    "\n",
    "4. **More neurons = more regions**: Networks with n neurons can have up to 2^n linear regions\n",
    "\n",
    "5. **Universal approximation**: Piecewise linear functions can approximate any continuous function with enough pieces\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "- **Interpretability**: Understanding which linear region an input falls into helps explain predictions\n",
    "- **Optimization**: The piecewise linear structure affects the optimization landscape\n",
    "- **Adversarial robustness**: Boundaries between regions are potential points of vulnerability\n",
    "- **Model compression**: Regions with similar linear equations can potentially be merged\n",
    "- **Theoretical understanding**: Connects deep learning to classical approximation theory\n",
    "\n",
    "### Going Deeper\n",
    "\n",
    "This property extends to:\n",
    "- Deeper networks (still piecewise linear!)\n",
    "- Other activation functions (Leaky ReLU, PReLU, Maxout)\n",
    "- Multi-output networks (each output is piecewise linear)\n",
    "- Modern architectures like ResNets (when using ReLU)\n",
    "\n",
    "**The piecewise linear structure is a fundamental geometric property of modern deep learning!**\n",
    "\n",
    "---\n",
    "\n",
    "## Further Exploration\n",
    "\n",
    "Try experimenting with:\n",
    "- Different numbers of neurons\n",
    "- Different random seeds\n",
    "- Deeper networks (add more layers)\n",
    "- Higher dimensional inputs\n",
    "- Different activation functions (e.g., Leaky ReLU)\n",
    "\n",
    "See the accompanying Python scripts for more examples and the OVERVIEW.md for theoretical details."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Piecewise Linear Demo",
   "language": "python",
   "name": "piecewise_linear_demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
